
Repository that stores datasets used in different COVID CEID projects. Repository will be made public in near future.  
=======
The datasets in the repository were compiled by members of the CEID COVID-19 working group. The data at the top level of the repository has been formated to be used 'as-is' and is updated often. Raw data and scripts are organized into sub-directories. The description of each data file along with the corresponding sub-directories are listed below.   

# Table of Contents

[UScases_by_state_wikipedia.csv](#uscases_by_state_wikipedia) </br>
[worldCases.csv](#worldcases) </br>
[China_casedata](#china_casedata) </br>

## UScases_by_state_wikipedia
Reads from wikipedia the number of non-repatriated COVID-19 cases in the US by state. Generated by `read_UScases_wikipedia.R` which extracts data from [2020 coronavirus pandemic in United States wiki page](https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_the_United_States) and outputs file 'UScases_by_state_wikipedia.csv'.

<b>Metadata: </b> </br>
 Column name description of data file:
- `Date` - Date correspondent to the number of cases
- Columns 2-14: Western US states
- Columns 15-27: Midwestern US states
- Columns 28-39: Southern US states
- Columns 40-51: Northeastern US states
- Column 52: GU: Guam, U.S territory
- Column 53: PR: Puerto Rico, U.S territory
- Column 54: VI: United States Virgin Islands, U.S territory
- `Conf_New`: Number of new confirmed cases 
- `Conf_Cml`: Cumulative number of confirmed cases
- `Death_New`: Number of new deaths
- `Death_Cml`: Cumulative number of deaths
- `Rec_New`: Number of new recoveries
- `Rec_Cml`: Cumulative number of recoveries
- `time_last_update`: day and time of last table update

<b>Related subdirectory and/or files </b>
- `read_UScases_wikipedia.R`
- UScases_by_state_wikipedia.csv

<b>Projects</b>
- TBD

## worldCases
Stores data read from wikipedia on the cumulative number of cases in a country. Added to each day by running `get-world-data.Rmd`to
extract data from [2020 coronavirus pandemic wiki page](https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic) and adds the current day to the dataset. Must be run every day at ~10 PM or extracted from archived pages on wayback machine. Wikipedia does not store these data by date so running daily at the same time is necessary to keep data current and accurate.

<b>Metadata:</b> </br>
 Column name description of data file:
 - `Date`: Date of cumulative case record
 - `Country`: Country name
 - `Cases`: cumulative number of cases for `Country` on `Date`

<b>Related subdirectory and/or files</b>
- `get-world-data.Rmd`
- worldCases.csv

<b>Projects</b>
- TBD

## China_casedata
Here is some discription

<b>Metadata:</b> 


<b>Related subdirectory and/or files</b>
- [China_casedata](https://github.com/CEIDatUGA/COVID-19-DATA/tree/master/China_casedata)

<b>Projects</b>
 - [Early Intervention](https://github.com/CEIDatUGA/ncov-early-intervention)
 - [Spatial spread in China](https://github.com/CEIDatUGA/CoronavirusSpatial)
 
## Data_name
Here is some discription of how the data is collected, when it is normally updated, etc. 

<b>Metadata:</b> 
 Column name description
 
<b>Related subdirectory and/or files</b>

<b>Projects</b>
List/Link related projects
# License 
TBD

Contact John Drake (jdrake@uga.edu) for questions. 

