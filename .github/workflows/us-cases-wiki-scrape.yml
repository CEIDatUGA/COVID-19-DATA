# This workflow updates any datasets that come from the dxy China scraping workflow

name: us-cases-wiki-scrape

# Controls when the action will run. Triggers the workflow on push or pull request
# events but only for the autoupdate branch
on:
  schedule:
    - cron : '0 2,10 * * *' # will run this script at 6 AM & 10 PM EST daily 

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "autoupdate"
  build:
    name: us-wiki-scrape
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
    # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
    - uses: actions/checkout@master
    #install R
    - uses: r-lib/actions/setup-r@master
    #Install curl 
    - name: install curl
      run: sudo apt-get install libcurl4-openssl-dev
    #Install R package dependencies
    - name: Install dependencies
      run: Rscript -e  "install.packages(c('xml2', 'rvest'), repos = 'https://cloud.r-project.org', dependencies = NA)" 
      
    #scrape data from github
    - name: Scrape data from Wikipedia
      run: Rscript  US/US_wikipedia_cases_fatalities/read_UScases_wikipedia.R

    #Push back to repo
    - name: Commit files
      run: |
        timestamp=$(date -u)
        git config --local user.email "actions@users.noreply.github.com"
        git config --local user.name "Automated Publisher"
        git commit -m "us-wiki autoupdate : ${timestamp} ${GITHUB_SHA}" -a
    - name: Push changes
      uses: ad-m/github-push-action@master
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        branch: master
