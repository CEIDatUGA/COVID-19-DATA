---
title: "get-world-data"
author: "Robbie Richards"
date: "3/16/2020"
output: html_document
---

RUN THIS SCRIPT AT 10 PM each day (or pull from archived site later)

xpath for the table on the wiki page is highly likely to change from day to day. In order to fix it you just:

1) Make sure the url is correct

2) Inspect the element in your browser and recopy the xpath



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
## Get Daily Country Data from Wikipedia
library(tidyverse)
library(googledrive)
library(googlesheets4)
library(cowplot)
library(emdbook)
library(plotly)
library(htmlwidgets)
library(crosstalk)
library(animation)
library(lubridate)
library(ggthemes)
library(crosstalk)
library(DT)
library(padr)
library(rvest)
library(tibbletime)
library(padr)
library(rvest)
library(tibbletime)
library(dplyr)
library(tidyr)
library(stringr)
```


```{r get hand scraped data, eval =F}
options(gargle_oauth_email = "robbielrichards@gmail.com")
df <- read_sheet("https://docs.google.com/spreadsheets/d/1SC28cM52m6s1gTJutpvFxadT9GGu-li90tsqkGuaM48/edit?usp=sharing", range = "Cumulative case reports by country - confirmed")

# Then strip out the extraneous columns)
dfStripped <- df[,-c(which(names(df) %in% c("Sources", "Contributor", "Notes")))]%>%
  drop_na()



dfLong <- pivot_longer(dfStripped, -Date, names_to = "Country", values_to = "Cases") %>%
  mutate(Date = as.character(Date)) %>% mutate(Country = replace(Country, which(dfLong$Country=="The Netherlands"), "Netherlands")) %>% mutate(Country = replace(Country, which(dfLong$Country=="USA"), "United States")) %>% mutate(Country = replace(Country, which(dfLong$Country=="St. Vincent and Grenadines"|"St. Vincent"), "St. Vincent and the Grenadines")) %>%  mutate(Country = replace(Country, which(dfLong$Country=="Democratic Republic of the Congo"), "DR Congo")) 
  
Netherlands

# df <- df %>% mutate(Country = replace(Country, which(df$Country=="St. Vincent and Grenadines"| df$Country=="St. Vincent"), "St. Vincent and the Grenadines")) %>%  mutate(Country = replace(Country, which(df$Country=="Democratic Republic of the Congo"), "DR Congo")) %>% mutate(Country = replace(Country, which(df$Country=="Netherlands (kingdom)[i]"), "Netherlands"))


write.csv(dfLong, "worldCases.csv")

```

```{r Extract archived day, eval =F}
df <- read.csv("worldCases.csv")


# Need to get link and div without wayback machine header using ctrl click on ubuntu

url <- "https://web.archive.org/web/20200315235036/https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic"
 World.wikipedia <- url %>%
   html() %>%
   html_nodes(xpath='/html/body/div[3]/div[3]/div[4]/div/div[6]/table') %>%
   html_table(fill = TRUE)
 World.wikipedia <- World.wikipedia[[1]]
 names(World.wikipedia) <- World.wikipedia[1,]
 World.wikipedia <- World.wikipedia[3:(nrow(World.wikipedia)-2),2:3]
 names(World.wikipedia)<- c("Country", "Cases")
 for(i in 1: nrow(World.wikipedia)){
   if(str_sub(World.wikipedia$Country[i], start= -1, end = -1)=="]"){
     World.wikipedia$Country[i] <- str_sub(World.wikipedia$Country[i], start = 1, end = -4)
   }
 }

 for(i in 1:nrow(World.wikipedia)){
   if(str_sub(World.wikipedia$Country[i], start = -1)=="]"){
     World.wikipedia$Country[i] <- str_sub(World.wikipedia$Country[i], end = -4)
   }
 }
 
 World.wikipedia$Date <- "2020-03-15"

 World.wikipedia$Cases <- as.numeric(as.character(gsub(",","" , World.wikipedia$Cases)))



dfNew <- bind_rows(df, World.wikipedia) %>% select(Date, Country, Cases)


write.csv(dfNew, "worldCases.csv")
```

```{r, eval =T}



df <- read.csv("worldCases.csv")%>%
  dplyr::select(Date, Country, Cases)

url <- "https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic_by_country_and_territory"
World.wikipedia <- url %>%
  html() %>%
  html_nodes(xpath='/html/body/div[3]/div[3]/div[4]/div/div[9]/table') %>%
  html_table(fill = TRUE)
World.wikipedia <- World.wikipedia[[1]]
names(World.wikipedia) <- World.wikipedia[1,]
World.wikipedia <- World.wikipedia[2:(nrow(World.wikipedia)-2),2:3]
names(World.wikipedia)<- c("Country", "Cases")
for(i in 1: nrow(World.wikipedia)){
  if(str_sub(World.wikipedia$Country[i], start= -1, end = -1)=="]"){
    World.wikipedia$Country[i] <- str_sub(World.wikipedia$Country[i], start = 1, end = -4)
  }
}

World.wikipedia$Date <- format(Sys.time(), '%Y-%m-%d')
# Then strip out the extraneous columns)


World.wikipedia$Cases <- as.numeric(as.character(gsub(",","" , World.wikipedia$Cases)))

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="Netherlands[l]"), "Netherlands")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="United States[f]"), "United States")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="Denmark[p][q]"), "Denmark")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="MS Zaandam["), "MS Zaandam")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="Somalia ["), "Somalia")
World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="Somalia["), "Somalia")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="MS Zaandam & Rotterdam["), "MS Zaandam & Rotterdam")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="Kosovo["), "Kosovo")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="Georgia["), "Georgia")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="French Guiana["), "French Guiana")

dfNew <- bind_rows(df, World.wikipedia) %>% dplyr::select(Date, Country, Cases) 

for(i in 1:nrow(dfNew)){
  
  if(str_sub(dfNew$Country[i], start=-1)=="["){
    dfNew$Country[i] <- str_sub(dfNew$Country[i], end =-2)
  }
}



### Check country names before writing!!!

write.csv(dfNew, "worldCases.csv")


```


### Fatalities



```{r get hand scraped data fatal, eval =T}
options(gargle_oauth_email = "robbielrichards@gmail.com")
df <- read_sheet("https://docs.google.com/spreadsheets/d/1SC28cM52m6s1gTJutpvFxadT9GGu-li90tsqkGuaM48/edit?usp=sharing", range = "Cumulative fatalities reported by country")

# Then strip out the extraneous columns)
dfStripped <- df[,-c(which(names(df) %in% c("Sources", "Contributor", "Notes")))]%>%
  drop_na()



dfLong <- pivot_longer(dfStripped, -Date, names_to = "Country", values_to = "Fatalities") 
dfLong <- dfLong %>%
  mutate(Date = as.character(Date)) %>% mutate(Country = replace(Country, which(dfLong$Country=="The Netherlands"), "Netherlands")) %>% mutate(Country = replace(Country, which(dfLong$Country=="USA"), "United States")) %>% mutate(Country = replace(Country, which(dfLong$Country=="St. Vincent and Grenadines" | dfLong$Country =="St. Vincent"), "St. Vincent and the Grenadines")) %>%  mutate(Country = replace(Country, which(dfLong$Country=="Democratic Republic of the Congo"), "DR Congo")) 
  

# df <- df %>% mutate(Country = replace(Country, which(df$Country=="St. Vincent and Grenadines"| df$Country=="St. Vincent"), "St. Vincent and the Grenadines")) %>%  mutate(Country = replace(Country, which(df$Country=="Democratic Republic of the Congo"), "DR Congo")) %>% mutate(Country = replace(Country, which(df$Country=="Netherlands (kingdom)[i]"), "Netherlands"))


write.csv(dfLong, "worldFatalities.csv")

```

```{r Extract archived day fatal, eval =F}
df <- read.csv("worldCases.csv")


# Need to get link and div without wayback machine header using ctrl click on ubuntu

url <- "https://web.archive.org/web/20200315235036/https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic"
 World.wikipedia <- url %>%
   html() %>%
   html_nodes(xpath='/html/body/div[3]/div[3]/div[4]/div/div[6]/table') %>%
   html_table(fill = TRUE)
 World.wikipedia <- World.wikipedia[[1]]
 names(World.wikipedia) <- World.wikipedia[1,]
 World.wikipedia <- World.wikipedia[3:(nrow(World.wikipedia)-2),2:3]
 names(World.wikipedia)<- c("Country", "Cases")
 for(i in 1: nrow(World.wikipedia)){
   if(str_sub(World.wikipedia$Country[i], start= -1, end = -1)=="]"){
     World.wikipedia$Country[i] <- str_sub(World.wikipedia$Country[i], start = 1, end = -4)
   }
 }

 for(i in 1:nrow(World.wikipedia)){
   if(str_sub(World.wikipedia$Country[i], start = -1)=="]"){
     World.wikipedia$Country[i] <- str_sub(World.wikipedia$Country[i], end = -4)
   }
 }
 
 World.wikipedia$Date <- "2020-03-15"

 World.wikipedia$Cases <- as.numeric(as.character(gsub(",","" , World.wikipedia$Cases)))



dfNew <- bind_rows(df, World.wikipedia) %>% select(Date, Country, Cases)


write.csv(dfNew, "worldCases.csv")
```

```{r, eval =F}



df <- read.csv("worldCases.csv")%>%
  dplyr::select(Date, Country, Cases)

url <- "https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic_by_country_and_territory"
World.wikipedia <- url %>%
  html() %>%
  html_nodes(xpath='/html/body/div[3]/div[3]/div[4]/div/div[9]/table') %>%
  html_table(fill = TRUE)
World.wikipedia <- World.wikipedia[[1]]
names(World.wikipedia) <- World.wikipedia[1,]
World.wikipedia <- World.wikipedia[2:(nrow(World.wikipedia)-2),2:3]
names(World.wikipedia)<- c("Country", "Cases")
for(i in 1: nrow(World.wikipedia)){
  if(str_sub(World.wikipedia$Country[i], start= -1, end = -1)=="]"){
    World.wikipedia$Country[i] <- str_sub(World.wikipedia$Country[i], start = 1, end = -4)
  }
}

World.wikipedia$Date <- format(Sys.time(), '%Y-%m-%d')
# Then strip out the extraneous columns)


World.wikipedia$Cases <- as.numeric(as.character(gsub(",","" , World.wikipedia$Cases)))

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="Netherlands[l]"), "Netherlands")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="United States[f]"), "United States")

World.wikipedia$Country <- replace(World.wikipedia$Country, which(World.wikipedia$Country=="Denmark[p][q]"), "Denmark")




dfNew <- bind_rows(df, World.wikipedia) %>% dplyr::select(Date, Country, Cases)


### Check country names before writing!!!

write.csv(dfNew, "worldCases.csv")


```
